#Informative features are extracted using the classifier in co-ordination of vectorizer
""" Return a list of the 'most informative' features used by this classifier. Let's say the informativeness of a feature
``(fname,fval)`` is equal to the highest value of P(fname=fval|label), for any label, divided by the lowest value of P(fname=fval|label),
for any label: max[ P(fname=fval|label1) / P(fname=fval|label2) ] """


def show_most_informative_features(model, vect, clf, text=None, n=50):
    # Extract the vectorizer and the classifier from the pipeline
    vectorizer = model.named_steps[vect]
    classifier = model.named_steps[clf]

     # Check to make sure that we can perform this computation
    if not hasattr(classifier, 'coef_'):
        raise TypeError(
            "Cannot compute most informative features on {}.".format(
                classifier.__class__.__name__
            )
        )
            
    if text is not None:
        # Compute the coefficients for the text
        tvec = model.transform([text]).toarray()
    else:
        # Otherwise simply use the coefficients
        tvec = classifier.coef_

    # Zip the feature names with the coefs and sort
    coefs = sorted(
        zip(tvec[0], vectorizer.get_feature_names()),
        reverse=True
    )
    
    # Get the top n and bottom n coef, name pairs
    topn  = zip(coefs[:n], coefs[:-(n+1):-1])

    # Create the output string to return
    output = []

    # If text, add the predicted value to the output.
    if text is not None:
        output.append("\"{}\"".format(text))
        output.append(
            "Classified as: {}".format(model.predict([text]))
        )
        output.append("")

    # Create two columns with most negative and most positive features.
    for (cp, fnp), (cn, fnn) in topn:
        output.append(
            "{:0.4f}{: >15}    {:0.4f}{: >15}".format(
                cp, fnp, cn, fnn
            )
        )
    print(output)
 
#From classification reports of final pipeline and N-gram pipeline we know that N-gram models are better 
 
show_most_informative_features(logR_pipeline_ngram,vect='LogR_tfidf',clf='LogR_clf')
show_most_informative_features(svm_pipeline_ngram,vect='svm_tfidf',clf='svm_clf')


#Visualization of the datasets using word clouds

def plot_show_wordcloud(dataset, title):    
    text = dataset['news'].values
    wordcloud = WordCloud(
        width=3000, height=2000, background_color='white',stopwords=STOPWORDS).generate(str(text))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.title('{}'.format(title))
    title = title.replace(' ', '_')
    plt.show()
    
    
def create_word_cloud(dataset, dataset_name):    
    title = 'The wordcloud of the complete ' + dataset_name
    plot_show_wordcloud(dataset, title)
    
    # Plotting the wordcloud for the true labels only
    true_dataset = dataset[dataset['label'].astype(str) == 'true']
    title = 'The wordcloud of the true labels of ' + dataset_name
    plot_show_wordcloud(true_dataset, title)
    
    # Plotting the wordcloud for the false labels only
    false_dataset = dataset[dataset['label'].astype(str) == 'false']
    title = 'The wordcloud of the false labels of ' + dataset_name
    plot_show_wordcloud(false_dataset, title)
    


create_word_cloud(train_news, 'Training news dataset')
create_word_cloud(test_news, 'Testing news dataset')
create_word_cloud(valid_news, 'Valid news dataset')
